{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 8: Cleaning the Archives\n",
    "## Handling Messy Data\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“º Video Walkthrough\n",
    "\n",
    "Watch the video tutorial for this notebook: [YouTube Link](#)\n",
    "\n",
    "---\n",
    "\n",
    "### The State of the Records\n",
    "\n",
    "*\"The problem with Quarry records,\" Chief Archivist Mink said, \"is that they're written by trappers. Half of them can't spell. The other half lie. And the ones who can spell and tell the truth tend to get eaten before they finish their reports.\"*\n",
    "\n",
    "*She dropped a stack of papers on the apprentice's desk. \"These are the raw field reports from the last expedition. Before you can analyze them, you need to clean them.\"*\n",
    "\n",
    "*\"Clean them?\"*\n",
    "\n",
    "*\"Fix the spelling. Fill in the blanks where you can. Mark the inconsistencies. Remove the duplicatesâ€”some crews report the same catch twice to inflate their numbers.\" She paused. \"And watch for the impossible. If someone reports catching a Grimslew Fish alive, they're either lying or about to discover their error.\"*\n",
    "\n",
    "*â€”From the training of an apprentice archivist*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What You Will Learn\n",
    "\n",
    "In this tutorial, you will learn to:\n",
    "\n",
    "1. Identify and handle missing values\n",
    "2. Find and remove duplicate records\n",
    "3. Fix inconsistent text (case, whitespace, typos)\n",
    "4. Validate data against known rules\n",
    "5. Convert data types appropriately\n",
    "\n",
    "By the end, you will be able to transform messy real-world data into clean, analyzable datasets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For this tutorial, we'll create a messy version of the catch data\n",
    "# In real life, you'd receive data in this state\n",
    "\n",
    "BASE_URL = \"https://raw.githubusercontent.com/buildLittleWorlds/yeller-quarry-data-science/main/data/\"\n",
    "\n",
    "# Load clean data to reference\n",
    "clean_catches = pd.read_csv(BASE_URL + \"catches.csv\")\n",
    "creatures = pd.read_csv(BASE_URL + \"creatures.csv\")\n",
    "\n",
    "print(\"Reference data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messy version for practice\n",
    "messy_catches = clean_catches.copy()\n",
    "\n",
    "# Introduce various problems:\n",
    "\n",
    "# 1. Missing values\n",
    "messy_catches.loc[3, 'quantity'] = np.nan\n",
    "messy_catches.loc[7, 'price_per_unit'] = np.nan\n",
    "messy_catches.loc[12, 'sector'] = np.nan\n",
    "messy_catches.loc[15, 'condition'] = np.nan\n",
    "messy_catches.loc[20, 'crew_id'] = np.nan\n",
    "\n",
    "# 2. Duplicate rows\n",
    "dup_row = messy_catches.iloc[5].copy()\n",
    "dup_row['catch_id'] = 'CAT0051'  # Different ID but same data\n",
    "messy_catches = pd.concat([messy_catches, pd.DataFrame([dup_row])], ignore_index=True)\n",
    "\n",
    "# Exact duplicate\n",
    "messy_catches = pd.concat([messy_catches, messy_catches.iloc[[10]]], ignore_index=True)\n",
    "\n",
    "# 3. Inconsistent text\n",
    "messy_catches.loc[2, 'sector'] = 'western marsh'  # lowercase\n",
    "messy_catches.loc[8, 'sector'] = 'Western  Marsh'  # extra space\n",
    "messy_catches.loc[14, 'sector'] = ' Western Marsh'  # leading space\n",
    "messy_catches.loc[18, 'condition'] = 'Live'  # capitalized\n",
    "messy_catches.loc[22, 'condition'] = 'DEAD'  # all caps\n",
    "\n",
    "# 4. Invalid values\n",
    "messy_catches.loc[25, 'quantity'] = -5  # negative quantity\n",
    "messy_catches.loc[28, 'price_per_unit'] = 0  # zero price (might be valid, might not)\n",
    "\n",
    "# 5. Wrong data types (strings where numbers should be)\n",
    "messy_catches['quantity'] = messy_catches['quantity'].astype(str)\n",
    "messy_catches.loc[30, 'quantity'] = 'three'  # text instead of number\n",
    "\n",
    "print(f\"Messy dataset created with {len(messy_catches)} rows\")\n",
    "print(\"Problems introduced: missing values, duplicates, inconsistent text, invalid values, wrong types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the messy data\n",
    "messy_catches.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Identifying Missing Values\n",
    "\n",
    "Missing values in pandas are represented as `NaN` (Not a Number) or `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "print(\"Missing values per column:\")\n",
    "print(messy_catches.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total missing values\n",
    "total_missing = messy_catches.isnull().sum().sum()\n",
    "print(f\"Total missing values: {total_missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which rows have missing values?\n",
    "rows_with_missing = messy_catches[messy_catches.isnull().any(axis=1)]\n",
    "print(f\"Rows with missing values: {len(rows_with_missing)}\")\n",
    "rows_with_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "You have several options:\n",
    "1. **Drop** rows or columns with missing values\n",
    "2. **Fill** with a specific value (0, mean, median, mode)\n",
    "3. **Forward/backward fill** (use previous or next value)\n",
    "4. **Leave as is** (some analyses handle NaN automatically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Drop rows with any missing values\n",
    "cleaned_v1 = messy_catches.dropna()\n",
    "print(f\"After dropna: {len(cleaned_v1)} rows (lost {len(messy_catches) - len(cleaned_v1)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Fill missing values\n",
    "cleaned_v2 = messy_catches.copy()\n",
    "\n",
    "# Fill missing sector with 'Unknown'\n",
    "cleaned_v2['sector'] = cleaned_v2['sector'].fillna('Unknown')\n",
    "\n",
    "# Fill missing condition with the mode (most common value)\n",
    "cleaned_v2['condition'] = cleaned_v2['condition'].fillna(clean_catches['condition'].mode()[0])\n",
    "\n",
    "print(\"After filling sector and condition:\")\n",
    "print(cleaned_v2.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Finding and Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows (all columns identical)\n",
    "duplicates = messy_catches.duplicated()\n",
    "print(f\"Exact duplicate rows: {duplicates.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the duplicates\n",
    "messy_catches[duplicates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates based on specific columns\n",
    "# (same trap, creature, date, quantity - probably same catch reported twice)\n",
    "subset_cols = ['trap_id', 'creature_id', 'date', 'quantity']\n",
    "potential_dups = messy_catches.duplicated(subset=subset_cols, keep=False)\n",
    "print(f\"Potential duplicate catches: {potential_dups.sum()}\")\n",
    "messy_catches[potential_dups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates - keep first occurrence\n",
    "deduped = messy_catches.drop_duplicates()\n",
    "print(f\"After removing exact duplicates: {len(deduped)} rows\")\n",
    "\n",
    "# Remove based on subset - keep first\n",
    "deduped_v2 = messy_catches.drop_duplicates(subset=subset_cols, keep='first')\n",
    "print(f\"After removing catch duplicates: {len(deduped_v2)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Fixing Inconsistent Text\n",
    "\n",
    "Text data often has inconsistencies: different cases, extra spaces, typos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at unique values in sector column\n",
    "print(\"Unique sector values (before cleaning):\")\n",
    "print(messy_catches['sector'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count variations\n",
    "messy_catches['sector'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see \"Western Marsh\" appears in multiple forms. Let's standardize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String cleaning chain\n",
    "cleaned = messy_catches.copy()\n",
    "\n",
    "# For string columns, apply cleaning operations\n",
    "cleaned['sector'] = (\n",
    "    cleaned['sector']\n",
    "    .str.strip()           # Remove leading/trailing whitespace\n",
    "    .str.lower()           # Convert to lowercase\n",
    "    .str.replace(r'\\s+', ' ', regex=True)  # Replace multiple spaces with single\n",
    "    .str.title()           # Convert to Title Case\n",
    ")\n",
    "\n",
    "print(\"After cleaning sector:\")\n",
    "print(cleaned['sector'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean condition column too\n",
    "cleaned['condition'] = cleaned['condition'].str.strip().str.lower()\n",
    "print(\"After cleaning condition:\")\n",
    "print(cleaned['condition'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Converting Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current data types\n",
    "print(messy_catches.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantity is a string! Let's see what values we have\n",
    "print(\"\\nQuantity values:\")\n",
    "print(messy_catches['quantity'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to convert to numeric - errors='coerce' turns unconvertible values to NaN\n",
    "cleaned['quantity'] = pd.to_numeric(cleaned['quantity'], errors='coerce')\n",
    "\n",
    "# See what couldn't be converted\n",
    "print(f\"Values that couldn't be converted: {cleaned['quantity'].isna().sum()}\")\n",
    "print(f\"New dtype: {cleaned['quantity'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the row with 'three'\n",
    "original_row = messy_catches[messy_catches['quantity'] == 'three']\n",
    "print(\"Row with text quantity:\")\n",
    "original_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could manually fix this if we know the correct value\n",
    "# For now, let's drop rows where quantity couldn't be converted\n",
    "cleaned = cleaned.dropna(subset=['quantity'])\n",
    "print(f\"Rows remaining: {len(cleaned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Validating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for invalid values\n",
    "\n",
    "# Negative quantities shouldn't exist\n",
    "negative_qty = cleaned[cleaned['quantity'] < 0]\n",
    "print(f\"Rows with negative quantity: {len(negative_qty)}\")\n",
    "negative_qty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero or negative prices might be errors\n",
    "invalid_price = cleaned[cleaned['price_per_unit'] <= 0]\n",
    "print(f\"Rows with zero/negative price: {len(invalid_price)}\")\n",
    "invalid_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate creature_ids against known creatures\n",
    "valid_creatures = creatures['creature_id'].tolist()\n",
    "invalid_creatures = cleaned[~cleaned['creature_id'].isin(valid_creatures)]\n",
    "print(f\"Rows with unknown creature_id: {len(invalid_creatures)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove invalid records\n",
    "validated = cleaned[\n",
    "    (cleaned['quantity'] > 0) & \n",
    "    (cleaned['price_per_unit'] > 0) &\n",
    "    (cleaned['creature_id'].isin(valid_creatures))\n",
    "]\n",
    "\n",
    "print(f\"After validation: {len(validated)} rows (removed {len(cleaned) - len(validated)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Creating a Cleaning Pipeline\n",
    "\n",
    "In practice, you'll want to combine all cleaning steps into a reusable function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_catch_data(df, creatures_ref):\n",
    "    \"\"\"\n",
    "    Clean catch data from the Yeller Quarry Archives.\n",
    "    \n",
    "    Steps:\n",
    "    1. Remove exact duplicates\n",
    "    2. Convert data types\n",
    "    3. Standardize text columns\n",
    "    4. Handle missing values\n",
    "    5. Validate against known constraints\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned DataFrame\n",
    "    \"\"\"\n",
    "    cleaned = df.copy()\n",
    "    initial_rows = len(cleaned)\n",
    "    \n",
    "    # Step 1: Remove exact duplicates\n",
    "    cleaned = cleaned.drop_duplicates()\n",
    "    print(f\"After removing duplicates: {len(cleaned)} rows\")\n",
    "    \n",
    "    # Step 2: Convert quantity to numeric\n",
    "    cleaned['quantity'] = pd.to_numeric(cleaned['quantity'], errors='coerce')\n",
    "    \n",
    "    # Step 3: Standardize text columns\n",
    "    text_cols = ['sector', 'condition', 'destination']\n",
    "    for col in text_cols:\n",
    "        if col in cleaned.columns:\n",
    "            cleaned[col] = (\n",
    "                cleaned[col]\n",
    "                .str.strip()\n",
    "                .str.lower()\n",
    "                .str.replace(r'\\s+', ' ', regex=True)\n",
    "                .str.title()\n",
    "            )\n",
    "    \n",
    "    # Condition should be lowercase\n",
    "    cleaned['condition'] = cleaned['condition'].str.lower()\n",
    "    \n",
    "    # Step 4: Handle missing values\n",
    "    # Drop rows missing critical fields\n",
    "    critical_cols = ['creature_id', 'quantity', 'date']\n",
    "    cleaned = cleaned.dropna(subset=critical_cols)\n",
    "    print(f\"After dropping rows with critical missing values: {len(cleaned)} rows\")\n",
    "    \n",
    "    # Fill non-critical missing values\n",
    "    cleaned['sector'] = cleaned['sector'].fillna('Unknown')\n",
    "    cleaned['condition'] = cleaned['condition'].fillna('unknown')\n",
    "    \n",
    "    # Step 5: Validate\n",
    "    valid_creatures = creatures_ref['creature_id'].tolist()\n",
    "    cleaned = cleaned[\n",
    "        (cleaned['quantity'] > 0) & \n",
    "        (cleaned['creature_id'].isin(valid_creatures))\n",
    "    ]\n",
    "    print(f\"After validation: {len(cleaned)} rows\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\nCleaning complete: {initial_rows} â†’ {len(cleaned)} rows\")\n",
    "    print(f\"Rows removed: {initial_rows - len(cleaned)}\")\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cleaning pipeline\n",
    "final_cleaned = clean_catch_data(messy_catches, creatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the results\n",
    "print(\"\\nFinal data quality check:\")\n",
    "print(f\"  Missing values: {final_cleaned.isnull().sum().sum()}\")\n",
    "print(f\"  Duplicate rows: {final_cleaned.duplicated().sum()}\")\n",
    "print(f\"  Unique sectors: {final_cleaned['sector'].nunique()}\")\n",
    "print(f\"  Unique conditions: {final_cleaned['condition'].nunique()}\")\n",
    "print(f\"  Quantity range: {final_cleaned['quantity'].min()} to {final_cleaned['quantity'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœï¸ Exercises\n",
    "\n",
    "### Exercise 1: Identify All Problems\n",
    "\n",
    "Before cleaning, write code to identify and count ALL the problems in the messy data:\n",
    "- Number of missing values per column\n",
    "- Number of duplicate rows\n",
    "- Number of unique variations of 'sector' and 'condition'\n",
    "- Number of invalid quantities (negative or non-numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Custom Mapping\n",
    "\n",
    "Sometimes you need to map incorrect values to correct ones. Create a mapping dictionary to fix common sector misspellings, then apply it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use df['column'].replace(mapping_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Date Validation\n",
    "\n",
    "Check that all dates in the catches are:\n",
    "1. Valid dates (can be parsed)\n",
    "2. Within the expected range (1855)\n",
    "3. Not in the future relative to the dataset's time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Outlier Detection\n",
    "\n",
    "Find potential outliers in the price_per_unit column using the IQR method (values below Q1 - 1.5*IQR or above Q3 + 1.5*IQR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Build Your Own Cleaner\n",
    "\n",
    "Write a function to clean the incidents data. Consider:\n",
    "- What columns are critical?\n",
    "- What values should be validated?\n",
    "- What text needs standardization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“‹ Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "| Concept | Code |\n",
    "|---------|------|\n",
    "| Check for missing | `df.isnull().sum()` |\n",
    "| Drop missing rows | `df.dropna()` |\n",
    "| Fill missing values | `df.fillna(value)` |\n",
    "| Find duplicates | `df.duplicated()` |\n",
    "| Remove duplicates | `df.drop_duplicates()` |\n",
    "| Strip whitespace | `df['col'].str.strip()` |\n",
    "| Change case | `df['col'].str.lower()` |\n",
    "| Replace text | `df['col'].str.replace()` |\n",
    "| Convert to numeric | `pd.to_numeric(df['col'], errors='coerce')` |\n",
    "| Validate values | `df[df['col'].isin(valid_list)]` |\n",
    "\n",
    "---\n",
    "\n",
    "## âž¡ï¸ Next Tutorial\n",
    "\n",
    "In **Tutorial 9: Seeing the Quarry**, you will learn to visualize your data using matplotlib and seabornâ€”creating charts that reveal patterns no table can show.\n",
    "\n",
    "*\"A chart is worth a thousand rows,\" Chief Archivist Mink said. \"The senators don't read tables. They look at pictures. Show them a line going up, and they'll fund your expedition. Show them a line going down, and they'll ask why you're still employed.\"*\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¾ Save Your Work\n",
    "\n",
    "**File > Save a copy in Drive** to keep your completed notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
